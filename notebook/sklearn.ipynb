{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[scikit-learn](http://scikit-learn.org/stable/index.html) is a free software machine learning library for the Python programming language. The [Getting Started guide](https://scikit-learn.org/stable/getting_started.html) illustrates some of the main features that `scikit-learn` provides.\n",
    "\n",
    "`scikit-learn` provides many built-in machine learning algorithms and models, called estimators. Each estimator can be fitted to some data using its `fit` method.\n",
    "\n",
    "Below is an example of fitting a Random Forest classifier to some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state = 0)\n",
    "\n",
    "# two samples and three features\n",
    "X = [[1, 2, 3],\n",
    "    [11, 12, 13]]\n",
    "\n",
    "# sample classes\n",
    "y = [0, 1]\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method generally accepts two inputs:\n",
    "\n",
    "1. The samples matrix `X`, where the size of `X` is typically `(n_samples, n_features)`.\n",
    "2. The target values `y` which are real numbers for regression or integers (or any other discrete set of values) for classification. `y` is usually a one-dimensional array where the `i`th entry corresponds to the target of the `i`th sample of `X`.\n",
    "\n",
    "Both `X` and `y` are usually expected to be `NumPy` arrays or equivalent array-like data types, though some estimators work with other formats such as sparse matrices.\n",
    "\n",
    "Once the estimator is fitted, it can be used for predicting target values of new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(\n",
    "    [[4, 5, 6], [14, 15, 16]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and pre-processors\n",
    "\n",
    "Machine learning workflows typically consists of a pre-processing step that transforms or imputes data. In `scikit-learn`, pre-processors and transformers follow the same API as the estimator objects (they actually all inherit from the same `BaseEstimator` class). The transformer objects don't have a `predict` method but rather a `transform` method that outputs a newly transformed sample matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [\n",
    "    [0, 15],\n",
    "    [1, -10]\n",
    "    ]\n",
    "\n",
    "# scale data according to computed scaling values\n",
    "StandardScaler().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining pre-processors and estimators\n",
    "\n",
    "Transformers and estimators (predictors) can be combined together into a single unifying object called a `Pipeline`. The pipeline offers the same API as a regular estimator: it can be fitted and used for prediction using `fit` and `predict`.\n",
    "\n",
    "In the following example, the iris dataset is split into training and testing sets, and the accuracy score is computed on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create a pipeline object\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "# load the iris dataset and split it into train and test sets\n",
    "X, y = load_iris(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# fit the whole pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# we can now use it like any other estimator\n",
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "`scikit-learn` provides many tools for model evaluation, in particular for cross-validation. The example below shows how to perform 5-fold cross-validation using the `cross_validate` helper. Note that it is also possible to manually iterate over the folds, select a different data splitting strategy, and use custom scoring functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "X, y = make_regression(n_samples=1000, random_state=0)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# default is 5-fold CV\n",
    "result = cross_validate(lr, X, y)\n",
    "result['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic parameter searches\n",
    "\n",
    "All estimators have parameters that can be tuned and the generalisation power of an estimator often critically depends on a few parameters. Quite often, it is not clear what the exact values of these parameters should be since they depend on the data. `scikit-learn` provides tools to automatically find the best parameter combinations (via cross-validation).\n",
    "\n",
    "In the following example, we randomly search over the parameter space of a Random Forest with a `RandomizedSearchCV` object. A `RandomForestRegressor` has a `n_estimators` parameter that determines the number of trees in the forest and a `max_depth` parameter that determines the maximum depth of each tree. When the search is over, the `RandomizedSearchCV` behaves as a `RandomForestRegressor` that has been fitted with the best set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'n_estimators': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "X, y = fetch_california_housing(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# define the parameter space that will be searched over\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(1, 5),\n",
    "    'max_depth': randint(5, 10)\n",
    "}\n",
    "\n",
    "# create a searchCV object and fit it to the data\n",
    "search = RandomizedSearchCV(\n",
    "    estimator = RandomForestRegressor(random_state = 0),\n",
    "    n_iter = 5,\n",
    "    param_distributions = param_distributions,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search object now acts like a normal random forest estimator with the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735363411343253"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "See the [User Guide](https://scikit-learn.org/stable/user_guide.html#user-guide) for details on all available tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
